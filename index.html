<!DOCTYPE html>
<style>

html, body {
	height: 100%;
}

body {
	margin: 0;
	position: relative;
}

#canvii {
	display: flex;
	flex-direction: column;
	align-items: center;
	justify-content: center;
	height: 100%;
	background: black;
}

#canvii > div {
	flex: 0;
	position: relative;
	margin: 4vh 0;
	height: 7vh;
}

#canvii > div > canvas {
	display: block;
	height: 100%;
	width: 100%;
}

#canvii canvas.flamen {
	position: absolute;
	bottom: 80%;
}

#mediaEl {
	position: absolute;
	bottom: 0;
}

#enableAudioContext {
  display: block;
  position: absolute;
  top: 0px;
  bottom: 0px;
  left: 0px;
  width: 100%;
  font: inherit;
  background: rgba(0, 0, 0, 0.7);
  color: white;
  border: none;
  outline: none;
  font-size: 2em;
  padding: 1em 1.5em;
}

#enableAudioContext:not(.visible) {
  display: none;
}

</style>
<!-- <audio id=mediaEl width=300 controls src="06 - What Does Your Soul Look Like (Pt. 4).mp3"></audio> -->
<!-- <video id=mediaEl width=300 controls src="The Rubens' Flame Tube - Seeing Sound Through Fire-pWekXMZJ2zM.mp4"></video> -->
<div id=canvii></div>
<button id=enableAudioContext>Click to start, please.</button>
<script id="vShaderEl" type="x-shader">
attribute vec3 p;

uniform float aspect;
uniform float time;

varying vec3 pos;

void main() {
	gl_Position = vec4(p, 1.0);
	pos = p;
}

</script>
<script id="fShaderEl" type="x-shader">
precision highp float;

uniform float time;
uniform sampler2D soundtex;

varying vec3 pos;

void main(void) {
	float brightness = texture2D(soundtex, vec2(pos.x / 2.0 + 0.5, 0.0))[0];

	gl_FragColor = vec4(
		brightness,
		brightness,
		brightness,
		1.0
	);
}

</script>
<script id="flameShaderEl" type="x-shader">
precision highp float;

uniform float time;
uniform sampler2D soundtex;

varying vec3 pos;

void main(void) {
	float br = 0.0;
	gl_FragColor = vec4(
		1.0,
		0.0,
		0.0,
		br
	);
}
</script>
<script>
'use strict';

class TubenCanvas {
	constructor(canvas, fShaderEl) {
		const gl = canvas.getContext('webgl');

		gl.pixelStorei(gl.UNPACK_ALIGNMENT, 1);

		const vertices = gl.createBuffer();
		gl.bindBuffer(gl.ARRAY_BUFFER, vertices);
		gl.bufferData(gl.ARRAY_BUFFER, new Float32Array([
			-1, 1, 0,
			-1, -1, 0,
			1, 1, 0,
			1, -1, 0,
		]), gl.STATIC_DRAW);

		const vShader = gl.createShader(gl.VERTEX_SHADER);
		gl.shaderSource(vShader, vShaderEl.text);
		gl.compileShader(vShader);

		const fShader = gl.createShader(gl.FRAGMENT_SHADER);
		gl.shaderSource(fShader, fShaderEl.text);
		gl.compileShader(fShader);

		const program = gl.createProgram();
		gl.attachShader(program, vShader);
		gl.attachShader(program, fShader);
		gl.linkProgram(program);

		gl.useProgram(program);

		this.timeLoc = gl.getUniformLocation(program, 'time');
		this.aspectLoc = gl.getUniformLocation(program, 'aspect');

		const pLoc = gl.getAttribLocation(program, 'p');
		gl.enableVertexAttribArray(pLoc);
		gl.vertexAttribPointer(pLoc, 3, gl.FLOAT, false, 0, 0);

		const soundTexLoc = gl.getAttribLocation(program, 'soundtex');
		var soundTexture = gl.createTexture();
		gl.bindTexture(gl.TEXTURE_2D, soundTexture);
		gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, gl.LINEAR);
		gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MAG_FILTER, gl.LINEAR);
		gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_S, gl.CLAMP_TO_EDGE);
		gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_T, gl.CLAMP_TO_EDGE);

		this.canvas = canvas;
		this.gl = gl;
		this.soundTexture = soundTexture;
		this.wasResized();
	}

	wasResized() {
		const clientRect = this.canvas.getBoundingClientRect();
		this.canvas.width = clientRect.width * devicePixelRatio;
		this.canvas.height = clientRect.height * devicePixelRatio;
		this.gl.uniform1f(this.aspectLoc, this.gl.drawingBufferWidth / this.gl.drawingBufferHeight);
		this.gl.viewport(0, 0, this.gl.drawingBufferWidth, this.gl.drawingBufferHeight);
	}

	draw(data, now) {
		const gl = this.gl;
		gl.bindTexture(gl.TEXTURE_2D, this.soundTexture);
		gl.texImage2D(gl.TEXTURE_2D, 0, gl.LUMINANCE, data.length, 1, 0, gl.LUMINANCE, gl.UNSIGNED_BYTE, data);
		gl.uniform1f(this.timeLoc, now / 1000);
		gl.drawArrays(gl.TRIANGLE_STRIP, 0, 4);
	}
}

class FlamenTuben {
	constructor(canvas, length) {
		this.canvas = new TubenCanvas(canvas, flameShaderEl);
		this.length = length;
	}

	pump(inData, now) {
		const outData = new Uint8Array(this.length);
		let ofs = inData.length - 1 - outData.length * 2;
		let max = 0;
		for (let i = ofs - 5; i >= 5; i--) {
			let sum = 0;
			for (let j = -5; j <= 5; j++)
				sum += inData[i + j];
			if (sum > max) {
				ofs = i;
				max = sum;
			}
		}
		for (let i = 0; i < outData.length; i++) {
			outData[i] = (inData[i + ofs] / 2) + (inData[outData.length * 2 + ofs - i] / 2);
		}
		this.canvas.draw(outData, now);
	}
}

class RubensTuben {
	constructor(canvas, length) {
		this.canvas = new TubenCanvas(canvas, fShaderEl);
		this.length = length;
	}

	pump(inData, now) {
		const outData = new Uint8Array(this.length);
		let ofs = inData.length - 1 - outData.length * 2;
		let max = 0;
		for (let i = ofs - 5; i >= 5; i--) {
			let sum = 0;
			for (let j = -5; j <= 5; j++)
				sum += inData[i + j];
			if (sum > max) {
				ofs = i;
				max = sum;
			}
		}
		for (let i = 0; i < outData.length; i++) {
			outData[i] = (inData[i + ofs] / 2) + (inData[outData.length * 2 + ofs - i] / 2);
		}
		this.canvas.draw(outData, now);
	}
}

// Safari 11 is still prefixed :/.
let audioContext = new (window.AudioContext || webkitAudioContext);

if (audioContext.state != 'running') {
  audioContext.addEventListener('statechange', e => {
    enableAudioContext.classList.remove('visible');
  }, { once: true});

  enableAudioContext.addEventListener('click', e => {
    audioContext.resume();
  }, { once: true });

  enableAudioContext.classList.add('visible');
}

// "Concert C": http://guerillascience.org/fire-organ-sculpture/
const tuning = 261.63
let tubens = [];

for (let i = 7; i >= 3; i--) {
	const length = Math.round(audioContext.sampleRate / tuning * i);
	const tubenEl = document.createElement('div');
	const tubeCanvas = document.createElement('canvas');
	const flameCanvas = document.createElement('canvas');

	tubenEl.style.width = `${i*12}%`;
	flameCanvas.classList.add('flamen');
	tubenEl.appendChild(tubeCanvas);
	tubenEl.appendChild(flameCanvas);
	canvii.appendChild(tubenEl);

	tubens.push(new RubensTuben(tubeCanvas, length));
	tubens.push(new FlamenTuben(flameCanvas, length));
}

window.addEventListener('resize', () => {
	for (const tuben of tubens)
		tuben.canvas.wasResized();
});

function pumpNode(node) {
	let analyser = audioContext.createAnalyser();
	try {
		analyser.fftSize = 4096;
	} catch (e) {
		console.error("Setting fftSize:", e);
	}
	node.connect(analyser);
	const draw = now => {
		const data = new Uint8Array(analyser.fftSize);
		analyser.getByteTimeDomainData(data);
		for (const tuben of tubens) {
			tuben.pump(data, now);
		}
		requestAnimationFrame(draw);
	};
	requestAnimationFrame(draw);
}

/*
// Tune the organ.
pumpNode((() => {
	let node = audioContext.createOscillator();
	node.frequency.value = 293.66;
	node.connect(audioContext.destination);
	node.start();
	{
		let rising = false;
		setInterval(() => {
			node.frequency.value *= rising ? 1.001 : 0.999;
			if (node.frequency.value > 440)
				rising = false;
			else if (node.frequency.value < 220)
				rising = true;
		}, 16);
	}
	return node;
})());
*/

navigator.getUserMedia({audio: true}, stream => {
	let node = audioContext.createMediaStreamSource(stream);
 	let gainz = audioContext.createGain();
 	gainz.gain.value = 4;
	node.connect(gainz);
	pumpNode(gainz);
}, err => { alert(`o shit: ${err}`); });

/*
let node = audioContext.createMediaElementSource(mediaEl);
let gainz = audioContext.createGain();
gainz.gain.value = 2;
node.connect(gainz);
node.connect(audioContext.destination);
pumpNode(gainz);
*/

</script>
